{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f48766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from falatra.utils import find_bbox_centre, distance_from_line\n",
    "from falatra.keypoints import Frame, FrameMatcher\n",
    "from falatra.model.head3d import deserialize_headmodel, HeadModel3D\n",
    "from falatra.model.stereo import StereoCalibration\n",
    "from falatra.markers import MarkerDetection\n",
    "import procrustes.utils\n",
    "from procrustes import generic\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c93e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listFullPaths(folder):\n",
    "    \n",
    "    paths = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        path = os.path.join(folder, filename)\n",
    "        paths.append(path)\n",
    "        \n",
    "    return paths\n",
    "\n",
    "training = {}\n",
    "training['side'] = listFullPaths('./data/training/sonny_ba/left')\n",
    "training['front'] = listFullPaths('./data/training/sonny_ba/centre')\n",
    "\n",
    "labels = {}\n",
    "labels['side'] = listFullPaths('./data/training/sonny_ba/left_labels')\n",
    "labels['front'] = listFullPaths('./data/training/sonny_ba/centre_labels')\n",
    "\n",
    "with open('./data/framefront.ser', 'rb') as fp:\n",
    "    modelframe_front = pickle.load(fp)\n",
    "    \n",
    "with open('./data/frameside.ser', 'rb') as fp:\n",
    "    modelframe_side = pickle.load(fp)\n",
    "\n",
    "headmodel_front = deserialize_headmodel('./data/headmodelfront.ser')\n",
    "headmodel_side  = deserialize_headmodel('./data/headmodelleft.ser')\n",
    "calibration = StereoCalibration()\n",
    "calibration.load('./data/calibration/calibration1')\n",
    "matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466bccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelframe_front.display()\n",
    "modelframe_side.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb9664b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653836966262983\n",
      "{'RightSide_ch': 0.0008523237473579557, 'LeftSide_ch': 0.0017814899434235832, 'Medial_m': 0.0013754545761547213, 'Medial_g': 0.0013302169167161704, 'Medial_prn': 0.0010822501093186567, 'Medial_pg': 0.0014583810811557428, 'Medial_ls': 0.0003550208974285491, 'RightSide_mvi': 0.0021438987122036866, 'RightSide_cph': 0.0006022809726044809, 'LeftSide_mvi': 0.002570711989227479, 'LeftSide_cph': 0.00031040050698610957}\n"
     ]
    }
   ],
   "source": [
    "def lowe_ratio_filter(knnmatches, loweratio=0.7):\n",
    "    \n",
    "    good = []\n",
    "    for knnmatch in knnmatches:\n",
    "        m, n = knnmatch[:2]  # get the 1st and 2nd closest\n",
    "        if m.distance < loweratio * n.distance:\n",
    "            good.append([m])\n",
    "            \n",
    "    return good\n",
    "\n",
    "def display_matches(matches, queryimg, querykps, trainimg, trainkps):\n",
    "    vis = cv2.drawMatchesKnn(queryimg, querykps, \n",
    "                             trainimg, trainkps, \n",
    "                             matches,\n",
    "                             None,\n",
    "                             flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure()\n",
    "    plt.imshow(vis[...,[2,1,0]])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_atom_coordinates(coords1, coords2,\n",
    "                          figsize=(12, 10),\n",
    "                          fontsize_label=14,\n",
    "                          fontsize_title=16,\n",
    "                          fontsize_legend=16,\n",
    "                          label1=None,\n",
    "                          label2=None,\n",
    "                          title=None,\n",
    "                          figfile=None):\n",
    "    \"\"\"Plot Cartesian coordinates of given atoms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords1: np.ndarray\n",
    "        Cartesian coordinates of given atom set 1.\n",
    "    coords2: np.ndarray\n",
    "        Cartesian coordinates of given atom set 2.\n",
    "    figsize : (float, float), optional\n",
    "        Figure size with width and height in inchies.\n",
    "    fontsize_label: int, optional\n",
    "        The font size for labels. Default=14.\n",
    "    fontsize_label: int, optional\n",
    "        The font size for title. Default=16.\n",
    "    label1 : str, optional\n",
    "        Label for coords1. Default=None.\n",
    "    label2 : str, optional\n",
    "        Label for coords2. Default=None.\n",
    "    title : str, optional\n",
    "        Figure title. Default=None.\n",
    "    figfile : str, optional\n",
    "        Figure file name to save it. Default=None.\n",
    "\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    ax.scatter(xs=coords1[:, 0], ys=coords1[:, 1], zs=coords1[:, 2],\n",
    "               marker=\"o\", color=\"blue\", s=40, label=label1)\n",
    "    ax.scatter(xs=coords2[:, 0], ys=coords2[:, 1], zs=coords2[:, 2],\n",
    "               marker=\"o\", color=\"red\", s=40, label=label2)\n",
    "\n",
    "    ax.set_xlabel(\"X\", fontsize=fontsize_label)\n",
    "    ax.set_ylabel(\"Y\", fontsize=fontsize_label)\n",
    "    ax.set_zlabel(\"Z\", fontsize=fontsize_label)\n",
    "    ax.legend(fontsize=fontsize_legend, loc=\"best\")\n",
    "\n",
    "    plt.title(title,\n",
    "              fontsize=fontsize_title)\n",
    "    # save figure to a file\n",
    "    if figfile:\n",
    "        plt.savefig(figfile)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "error_dict = {}\n",
    "errors = []\n",
    "end = len(training['side']) - 1\n",
    "for image_index in range(1):\n",
    "\n",
    "    image_front = cv2.imread(training['front'][image_index])\n",
    "    label_front = MarkerDetection()\n",
    "    label_front.load(labels['front'][image_index])\n",
    "    frame_front = Frame(image_front)\n",
    "    frame_front.detect(detectFace=True)\n",
    "\n",
    "    image_side = cv2.imread(training['side'][image_index])\n",
    "    label_side = MarkerDetection()\n",
    "    label_side.load(labels['side'][image_index])\n",
    "    frame_side = Frame(image_side)\n",
    "    frame_side.detect(detectFace=True)\n",
    "    \n",
    "    # ---------- 1st step: FIND CORRESPONDENCES BETWEEN STEREO IMAGES ---------- \n",
    "\n",
    "    stereomatches = matcher.knnMatch(frame_side.des, frame_front.des, 2)\n",
    "\n",
    "    # Lowe ratio filtering\n",
    "    loweratio = 0.9\n",
    "    stereomatches = lowe_ratio_filter(stereomatches, loweratio)\n",
    "\n",
    "    # Epipolar constraint\n",
    "    epipolar_threshold = 20.0\n",
    "    good = []\n",
    "    for match in stereomatches:\n",
    "\n",
    "        pt_src = frame_side.kps[match[0].queryIdx].pt\n",
    "        pt_dst = frame_front.kps[match[0].trainIdx].pt\n",
    "\n",
    "        # undistort these points\n",
    "        pt_src = calibration.undistort_points([pt_src], view='right').squeeze()\n",
    "        pt_dst = calibration.undistort_points([pt_dst], view='left').squeeze()\n",
    "\n",
    "        pt_dst = pt_dst[np.newaxis, ...]  # junk to make this work if my code wasn't so bad\n",
    "        line = calibration.compute_correspond_epilines(pt_dst, view='right').squeeze()\n",
    "\n",
    "        d = distance_from_line(pt_src, *line)\n",
    "        if d < epipolar_threshold:\n",
    "            good.append(match)\n",
    "    stereomatches = good\n",
    "\n",
    "    # display result\n",
    "    #display_matches(stereomatches, frame_side.image, frame_side.kps, frame_front.image, frame_front.kps)\n",
    "\n",
    "    # ---------- 2nd step: FIND CORRESPONDENCES BETWEEN IMAGE AND HEAD MODEL ---------- \n",
    "    modelmatches_centre = matcher.knnMatch(frame_front.des, headmodel_front.descriptors, 2)\n",
    "    modelmatches_side = matcher.knnMatch(frame_side.des, headmodel_side.descriptors, 2)\n",
    "\n",
    "    # Lowe ratio filtering\n",
    "    loweratio = 0.95\n",
    "    modelmatches_centre = lowe_ratio_filter(modelmatches_centre, loweratio)\n",
    "    #display_matches(modelmatches_centre, frame_front.image, frame_front.kps, modelframe_front.image, modelframe_front.kps)\n",
    "\n",
    "\n",
    "    # ---------- 3rd step: 3-WAY MATCHING BETWEEN STEREO MODEL & VECTRA 3D MODEL ----------\n",
    "\n",
    "    highfi_model = HeadModel3D()\n",
    "    lowfi_model = HeadModel3D()\n",
    "\n",
    "\n",
    "    #O(N^2), can be improved to O(Nlog) if we use a more efficient biparte graph matching or\n",
    "    # using a hash map to store stereo matches...\n",
    "    new_modelmatches_centre = []\n",
    "    for match in modelmatches_centre:\n",
    "\n",
    "        stereoimg_indx = match[0].queryIdx  \n",
    "        for match2 in stereomatches:\n",
    "            if stereoimg_indx == match2[0].trainIdx:\n",
    "                new_modelmatches_centre.append(match)\n",
    "\n",
    "                # triangulate keypoint point\n",
    "                pt_src = frame_side.kps[match2[0].queryIdx].pt\n",
    "                pt_dst = frame_front.kps[match2[0].trainIdx].pt\n",
    "\n",
    "                # undistort these points\n",
    "                pt_src = calibration.undistort_points([pt_src], view='right').squeeze()\n",
    "                pt_dst = calibration.undistort_points([pt_dst], view='left').squeeze()\n",
    "\n",
    "                X, X1, X2 = calibration.triangulate(pt_dst, pt_src) # src and dst is swap because calibration inverse camera definitions\n",
    "                lowfi_model.addFeaturePoint(X1.flatten(), frame_side.des[match2[0].queryIdx])\n",
    "\n",
    "                # create a new model from highfi model that only contain correspondence keypoints\n",
    "                highfi_model.addFeaturePoint(headmodel_front.keypoints[match[0].trainIdx],\n",
    "                                             headmodel_front.descriptors[match[0].trainIdx])\n",
    "\n",
    "    # Triangulate landmarks\n",
    "    for name in label_front.bboxes.keys():\n",
    "        if name in label_side.bboxes.keys():\n",
    "\n",
    "            # Triangulate landmark\n",
    "            pt_src = find_bbox_centre(label_side.bboxes[name])\n",
    "            pt_dst = find_bbox_centre(label_front.bboxes[name])\n",
    "\n",
    "            # undistort these points\n",
    "            pt_src = calibration.undistort_points([pt_src], view='right').squeeze()\n",
    "            pt_dst = calibration.undistort_points([pt_dst], view='left').squeeze()\n",
    "\n",
    "            X, X1, X2 = calibration.triangulate(pt_dst, pt_src) # src and dst is swap because calibration inverse camera\n",
    "            lowfi_model.setLandmark(name, X1)\n",
    "\n",
    "\n",
    "    # Transfer landmarks to highfi mode\n",
    "    for landmark in lowfi_model._landmarks.keys():\n",
    "        if landmark in headmodel_front._landmarks:\n",
    "            highfi_model.setLandmark(landmark, headmodel_front._landmarks[landmark])\n",
    "\n",
    "\n",
    "    #display_matches(new_modelmatches_centre, frame_front.image, frame_front.kps, modelframe_front.image, modelframe_front.kps)       \n",
    "\n",
    "    # ---------- 4th step: Procrustes analysis ----------\n",
    "        \n",
    "    result = generic(highfi_model.keypoints, lowfi_model.keypoints, translate=True, scale=True)\n",
    "    kp_new = np.dot(result.new_a, result.t)\n",
    "    \"\"\"\n",
    "    plot_atom_coordinates(coords1=kp_new,\n",
    "                          coords2=result.new_b,\n",
    "                          figsize=(8, 6),\n",
    "                          label1=\"Highfi model points\",\n",
    "                          label2=\"Lowfi model points\",\n",
    "                          title=\"Keypoint registration\")\n",
    "    \"\"\"\n",
    "    lndmk_ref = []  # reference landmarks from highfi model\n",
    "    lndmk_gt = []   # ground truth \n",
    "    for landmark in lowfi_model._landmarks.keys():\n",
    "        lndmk_gt.append(np.array(lowfi_model._landmarks[landmark]).flatten())\n",
    "        lndmk_ref.append(np.array(highfi_model._landmarks[landmark]).flatten())\n",
    "\n",
    "    lndmk_gt = np.array(lndmk_gt)\n",
    "    lndmk_ref = np.array(lndmk_ref)\n",
    "\n",
    "    # normalisation\n",
    "    lndmk_gt, lndmk_ref = procrustes.utils.setup_input_arrays(lndmk_gt, lndmk_ref, remove_zero_col=False,\n",
    "                                                         remove_zero_row=False,\n",
    "                                                         pad=False, translate=True, scale=True,\n",
    "                                                         check_finite=True, weight=None)\n",
    "\n",
    "    # registration\n",
    "    lndmk_inferred = np.dot(lndmk_ref, result.t)\n",
    "    \"\"\"\n",
    "    plot_atom_coordinates(coords1=lndmk_inferred,\n",
    "                          coords2=lndmk_gt,\n",
    "                          figsize=(8, 6),\n",
    "                          label1=\"Inferred Landmarks\",\n",
    "                          label2=\"Ground-truth landmarks\",\n",
    "                          title=\"Landmarks registration\")\n",
    "    \"\"\"\n",
    "    rmsd= np.sqrt(np.mean(np.sum((lndmk_ref - lndmk_inferred)**2, axis=1)))\n",
    "    print(rmsd)\n",
    "    errors.append(rmsd)\n",
    "    \n",
    "    for i, landmark in enumerate(lowfi_model._landmarks.keys()):\n",
    "        pt3d_1 = lndmk_inferred[i]\n",
    "        pt3d_2 = lndmk_gt[i]\n",
    "        \n",
    "        if landmark not in error_dict:\n",
    "            error_dict[landmark] = 0\n",
    "            \n",
    "        error_dict[landmark] += np.sqrt(np.sum((pt3d_1 - pt3d_2)**2))\n",
    "        \n",
    "for key, value in error_dict.items():\n",
    "    error_dict[key] = value / len(training['side'])\n",
    "    \n",
    "print(error_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a9ab9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.92, 'Landmark Registration')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(xs=lndmk_inferred[:, 0], ys=lndmk_inferred[:, 1], zs=lndmk_inferred[:, 2],\n",
    "               marker=\"o\", color=\"blue\", s=20, label=\"Landmarks inferred\")\n",
    "ax.scatter(xs=lndmk_gt[:, 0], ys=lndmk_gt[:, 1], zs=lndmk_gt[:, 2],\n",
    "               marker=\"o\", color=\"red\", s=20, label=\"Landmarks ground-truth\")\n",
    "\n",
    "for i, landmark in enumerate(lowfi_model._landmarks.keys()):\n",
    "    pt3d = lndmk_inferred[i]\n",
    "    ax.text(*pt3d, f\"{landmark}\", color='blue', fontsize='xx-small')\n",
    "    pt3d = lndmk_gt[i]\n",
    "    ax.text(*pt3d, f\"{landmark}\", color='red', fontsize='xx-small')\n",
    "    \n",
    "    \n",
    "ax.set_xlabel(\"X\", fontsize=8)\n",
    "ax.set_ylabel(\"Y\", fontsize=8)\n",
    "ax.set_zlabel(\"Z\", fontsize=8)\n",
    "ax.legend(fontsize=8, loc=\"best\")\n",
    "\n",
    "plt.title(\"Landmark Registration\",\n",
    "          fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37778d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(range(len(errors)),errors)\n",
    "plt.xlabel('Image pair')\n",
    "plt.xticks(range(len(errors)))\n",
    "plt.ylabel('Root-Mean Squared Error')\n",
    "plt.title('Registration Error normalised')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a0d6288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Normalised L2 Error')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "ax.barh(list(error_dict.keys()),error_dict.values(), align='center')\n",
    "ax.set_title('Average L2 error for each landmark of one participant')\n",
    "ax.set_xlabel('Normalised L2 Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578790f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cb2bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
